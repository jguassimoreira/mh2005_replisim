#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data", "replication") #path where we'll eventually save our data
vcov.default = function(object,...) object$vcov #temp
#########
## Define the simulation conditions
#########
#these values can be found on the right hand column of M&H2005 pg 88
icc_conds = c(0.1, 0.2, 0.3) #the ICC levels
ngroup_conds = c(30, 50, 100) #L2 sample size levels (using diff var name to differentiate from function vars)
groupsize_conds = c(5, 30, 50) #L1 sample size levels (same note re naming; this naming is consistent w M&H paper)
sim_conds = expand.grid(icc_conds, ngroup_conds, groupsize_conds); names(sim_conds) = c('icc', 'ngroups', 'groupsize') #create dataframe with all conditions
#########
## Generate the x and z data
#########
set.seed(1234) #setting a seed for reproducibility
#M&H used a fixed dataset for their sims since multilevel models assume explanatory variables are fixed
zdat = data.frame(subID = c(paste0('sub',1:30)), z = rnorm(30)) #L2/between cluster variables
xdat = data.frame(subID = rep(paste0('sub',1:30), each = 5), x = rnorm(5*30)) #L1/within cluster variables
#########
## Run the simulation
#########
for (i in 1:dim(sim_conds)[1]) { #loop over the simulations
print(sim_conds[i,])#print current condition
outList = list() #initialize empty list to hold output
for (r in 1:1000) { #1000 replications per condition, as specified by M&H, pg 89
outList[[r]] = sim_mlm_legacy(N_l2 = sim_conds[i, 'ngroups'], #simulate data, save output
n_l1 = sim_conds[i, 'groupsize'],
icc = sim_conds[i, 'icc'],
sigma2 = 0.5,
xdat = xdat,
zdat = zdat)
names(outList[[r]]) = c('fx_coefs', 'varComp', 'confInt', 'w')
if((r / 100) == round(r / 100)) {print(sprintf('completed iteration %s', r))} #notify us of the sims progress every hundred reps
}
saveRDS(outList, file = sprintf('%s/icc%s_ngrps%s_grpsize%s', datPath, sim_conds[i,'icc']*10, sim_conds[i,'ngroups'], sim_conds[i,'groupsize']))
}
warnings()
library(ggplot2)
library(aod)
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data") #path where we'll eventually save our data
#########
## Analyze the sim
#########
#read in all the files
conds_to_load = Sys.glob(file.path(sprintf("%s/%s", datPath, "replication"), "*.RDS")) #get file paths for all M&H simulation conditions
conds_list = lapply(conds_to_load, FUN = readRDS) #read in the conditions
params_to_analyze = c("int_var", "x_var", "sigma2", "intercept", "b_x", "b_z", "b_xz") #analyze the parameters
rm(list=ls())
library(ggplot2)
library(aod)
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data") #path where we'll eventually save our data
#########
## Analyze the sim
#########
#read in all the files
conds_to_load = Sys.glob(file.path(sprintf("%s/%s", datPath, "replication"), "*.RDS")) #get file paths for all M&H simulation conditions
conds_list = lapply(conds_to_load, FUN = readRDS) #read in the conditions
params_to_analyze = c("int_var", "x_var", "sigma2", "intercept", "b_x", "b_z", "b_xz") #analyze the parameters
simList = conds_list[[1]]
conds = conds_to_load
conds
simList[[1]]
#extract icc conditions
icc_conds = lapply(conds, function(x) parse_conds(x)[[1]])
simList = simList[[1]]
simList
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
simList$varComp
simList$varComp[simList$varComp$grp == 'Residual', 'vcov']
simList = conds_list
simList = conds_list[[1]]
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
sims
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['.sigma',])
ci_mat
simList[[1]]$confInt
?lme4
?lmer
fm1 = fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
fm1
confint(fm1)
confint(fm1, method = "Wald")
compute_wald_ci_varcomp(fm1)
compute_wald_ci_varcomp()
compute_wald_ci_varcomp
compute_wald_ci_varcomp(fm1)
lme4::devfun2(fm1,useSc=TRUE,signames=FALSE)
dd = lme4::devfun2(fm1,useSc=TRUE,signames=FALSE)
dd
vv = as.data.frame(lme4::VarCorr(fm1))
vv
vnames = c(sprintf(".sig%02d",1:3),".sigma")
vnames
lme4::confint.merMod(mod, method ="Wald", oldNames = F, quiet = T)
lme4::confint.merMod(fm1, method ="Wald", oldNames = F, quiet = T)
compute_wald_ci_varcomp(fm1)
compute_wald_ci_varcomp
mod = fm1
mod
dd = lme4::devfun2(model,useSc=TRUE,signames=FALSE)
model = mod
dd = lme4::devfun2(model,useSc=TRUE,signames=FALSE)
vv = as.data.frame(lme4::VarCorr(model))
vnames = c(sprintf(".sig%02d",1:3),".sigma")
pars = setNames(vv[c(1,3,2,4),"sdcor"],vnames)
hh1 = numDeriv::hessian(dd,pars)
vv2 = 2*solve(hh1)
dimnames(vv2) = list(vnames,vnames)
dimnames
vv2
vhack = list(coefficients=pars,vcov=vv2)
vhack
wci = confint.default(vhack)
vhack
vhack
?confint.default
confint.default(vhack)
confint.default(vhack)
confint(vhack)
confint.default(vhack)
confint.default(vhack$coefficients)
simLlist[[1]]
simList[[1]]
model
compute_wald_ci_varcomp(model)
rm(list=ls())
########################################################################
###################### M&H2005 Replication Script ######################
########### JF Guassi Moreira (jguassimoreira[at]ucla[dot]edu) #########
########################### Winter 2021/22 #############################
#########
## Documentation
#########
#This is the main script for replicating M&H's 2005 findings
#the following libraries are called (python style), not loaded:
#dplyr
#lme4
#lmerTest
#MASS
#these packages are used in the helper functions used in this script
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data", "replication") #path where we'll eventually save our data
vcov.default = function(object,...) object$vcov #temp
fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
fm1
compute_wald_ci_varcomp(model)
compute_wald_ci_varcomp(fm1)
library(ggplot2)
library(aod)
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data") #path where we'll eventually save our data
#########
## Analyze the sim
#########
#read in all the files
conds_to_load = Sys.glob(file.path(sprintf("%s/%s", datPath, "replication"), "*.RDS")) #get file paths for all M&H simulation conditions
conds_list = lapply(conds_to_load, FUN = readRDS) #read in the conditions
params_to_analyze = c("int_var", "x_var", "sigma2", "intercept", "b_x", "b_z", "b_xz") #analyze the parameters
conds_list[[1]][[1]]$confInt
confint(fm1)
confint(fm1, method = "Wald")
confint(fm1, method = "Wald")['(Intercept)',]
rm(list=ls())
rm(list=ls())
########################################################################
###################### M&H2005 Replication Script ######################
########### JF Guassi Moreira (jguassimoreira[at]ucla[dot]edu) #########
########################### Winter 2021/22 #############################
#########
## Documentation
#########
#This is the main script for replicating M&H's 2005 findings
#the following libraries are called (python style), not loaded:
#dplyr
#lme4
#lmerTest
#MASS
#these packages are used in the helper functions used in this script
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data", "replication") #path where we'll eventually save our data
vcov.default = function(object,...) object$vcov #temp
#########
## Define the simulation conditions
#########
#these values can be found on the right hand column of M&H2005 pg 88
icc_conds = c(0.1, 0.2, 0.3) #the ICC levels
ngroup_conds = c(30, 50, 100) #L2 sample size levels (using diff var name to differentiate from function vars)
groupsize_conds = c(5, 30, 50) #L1 sample size levels (same note re naming; this naming is consistent w M&H paper)
sim_conds = expand.grid(icc_conds, ngroup_conds, groupsize_conds); names(sim_conds) = c('icc', 'ngroups', 'groupsize') #create dataframe with all conditions
#########
## Generate the x and z data
#########
set.seed(1234) #setting a seed for reproducibility
#M&H used a fixed dataset for their sims since multilevel models assume explanatory variables are fixed
zdat = data.frame(subID = c(paste0('sub',1:30)), z = rnorm(30)) #L2/between cluster variables
xdat = data.frame(subID = rep(paste0('sub',1:30), each = 5), x = rnorm(5*30)) #L1/within cluster variables
#########
## Run the simulation
#########
for (i in 1:dim(sim_conds)[1]) { #loop over the simulations
print(sim_conds[i,])#print current condition
outList = list() #initialize empty list to hold output
for (r in 1:1000) { #1000 replications per condition, as specified by M&H, pg 89
outList[[r]] = sim_mlm_legacy(N_l2 = sim_conds[i, 'ngroups'], #simulate data, save output
n_l1 = sim_conds[i, 'groupsize'],
icc = sim_conds[i, 'icc'],
sigma2 = 0.5,
xdat = xdat,
zdat = zdat)
names(outList[[r]]) = c('fx_coefs', 'varComp', 'confInt', 'w')
if((r / 100) == round(r / 100)) {print(sprintf('completed iteration %s', r))} #notify us of the sims progress every hundred reps
}
saveRDS(outList, file = sprintf('%s/icc%s_ngrps%s_grpsize%s', datPath, sim_conds[i,'icc']*10, sim_conds[i,'ngroups'], sim_conds[i,'groupsize']))
}
rm(list=ls())
########################################################################
###################### M&H2005 Replication Script ######################
########### JF Guassi Moreira (jguassimoreira[at]ucla[dot]edu) #########
########################### Winter 2021/22 #############################
#########
## Documentation
#########
#This is the main script for replicating M&H's 2005 findings
#the following libraries are called (python style), not loaded:
#dplyr
#lme4
#lmerTest
#MASS
#these packages are used in the helper functions used in this script
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data", "replication") #path where we'll eventually save our data
vcov.default = function(object,...) object$vcov #temp
#########
## Define the simulation conditions
#########
#these values can be found on the right hand column of M&H2005 pg 88
icc_conds = c(0.1, 0.2, 0.3) #the ICC levels
ngroup_conds = c(30, 50, 100) #L2 sample size levels (using diff var name to differentiate from function vars)
groupsize_conds = c(5, 30, 50) #L1 sample size levels (same note re naming; this naming is consistent w M&H paper)
sim_conds = expand.grid(icc_conds, ngroup_conds, groupsize_conds); names(sim_conds) = c('icc', 'ngroups', 'groupsize') #create dataframe with all conditions
#########
## Generate the x and z data
#########
set.seed(1234) #setting a seed for reproducibility
#M&H used a fixed dataset for their sims since multilevel models assume explanatory variables are fixed
zdat = data.frame(subID = c(paste0('sub',1:30)), z = rnorm(30)) #L2/between cluster variables
xdat = data.frame(subID = rep(paste0('sub',1:30), each = 5), x = rnorm(5*30)) #L1/within cluster variables
#########
## Run the simulation
#########
for (i in 1:dim(sim_conds)[1]) { #loop over the simulations
print(sim_conds[i,])#print current condition
outList = list() #initialize empty list to hold output
for (r in 1:1000) { #1000 replications per condition, as specified by M&H, pg 89
outList[[r]] = sim_mlm_legacy(N_l2 = sim_conds[i, 'ngroups'], #simulate data, save output
n_l1 = sim_conds[i, 'groupsize'],
icc = sim_conds[i, 'icc'],
sigma2 = 0.5,
xdat = xdat,
zdat = zdat)
names(outList[[r]]) = c('fx_coefs', 'varComp', 'confInt', 'w')
if((r / 100) == round(r / 100)) {print(sprintf('completed iteration %s', r))} #notify us of the sims progress every hundred reps
}
saveRDS(outList, file = sprintf('%s/icc%s_ngrps%s_grpsize%s.RDS', datPath, sim_conds[i,'icc']*10, sim_conds[i,'ngroups'], sim_conds[i,'groupsize']))
}
rm(list=ls())
########################################################################
###################### M&H2005 Replication Script ######################
########### JF Guassi Moreira (jguassimoreira[at]ucla[dot]edu) #########
########################### Winter 2021/22 #############################
#########
## Documentation
#########
#This is the main script for replicating M&H's 2005 findings
#the following libraries are called (python style), not loaded:
#dplyr
#lme4
#lmerTest
#MASS
#these packages are used in the helper functions used in this script
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data", "replication") #path where we'll eventually save our data
vcov.default = function(object,...) object$vcov #temp
#########
## Define the simulation conditions
#########
#these values can be found on the right hand column of M&H2005 pg 88
icc_conds = c(0.1, 0.2, 0.3) #the ICC levels
ngroup_conds = c(30, 50, 100) #L2 sample size levels (using diff var name to differentiate from function vars)
groupsize_conds = c(5, 30, 50) #L1 sample size levels (same note re naming; this naming is consistent w M&H paper)
sim_conds = expand.grid(icc_conds, ngroup_conds, groupsize_conds); names(sim_conds) = c('icc', 'ngroups', 'groupsize') #create dataframe with all conditions
#########
## Generate the x and z data
#########
set.seed(1234) #setting a seed for reproducibility
#M&H used a fixed dataset for their sims since multilevel models assume explanatory variables are fixed
zdat = data.frame(subID = c(paste0('sub',1:30)), z = rnorm(30)) #L2/between cluster variables
xdat = data.frame(subID = rep(paste0('sub',1:30), each = 5), x = rnorm(5*30)) #L1/within cluster variables
#########
## Run the simulation
#########
for (i in 1:dim(sim_conds)[1]) { #loop over the simulations
print(sim_conds[i,])#print current condition
outList = list() #initialize empty list to hold output
for (r in 1:1000) { #1000 replications per condition, as specified by M&H, pg 89
outList[[r]] = sim_mlm_legacy(N_l2 = sim_conds[i, 'ngroups'], #simulate data, save output
n_l1 = sim_conds[i, 'groupsize'],
icc = sim_conds[i, 'icc'],
sigma2 = 0.5,
xdat = xdat,
zdat = zdat)
names(outList[[r]]) = c('fx_coefs', 'varComp', 'confInt', 'w')
if((r / 100) == round(r / 100)) {print(sprintf('completed iteration %s', r))} #notify us of the sims progress every hundred reps
}
saveRDS(outList, file = sprintf('%s/icc%s_ngrps%s_grpsize%s.RDS', datPath, sim_conds[i,'icc']*10, sim_conds[i,'ngroups'], sim_conds[i,'groupsize']))
}
rm(list=ls())
library(ggplot2)
library(aod)
#########
## Import helper functions, set path to data
#########
studyPath = file.path("/Users", "jguassimoreira", "Documents", "mh2005_replisim") #declare the path to the study
scriptPath = file.path(sprintf("%s", studyPath), "funcs") #path to helper funcs
file.sources = Sys.glob(file.path(sprintf("%s", scriptPath), "*.R")) #Construct paths to helper funcs, save in vector
invisible(sapply(file.sources,FUN=source)) #use sapply to source each file path in the helper func vector
datPath = file.path(sprintf("%s", studyPath), "data") #path where we'll eventually save our data
#########
## Analyze the sim
#########
#read in all the files
conds_to_load = Sys.glob(file.path(sprintf("%s/%s", datPath, "replication"), "*.RDS")) #get file paths for all M&H simulation conditions
conds_list = lapply(conds_to_load, FUN = readRDS) #read in the conditions
params_to_analyze = c("int_var", "x_var", "sigma2", "intercept", "b_x", "b_z", "b_xz") #analyze the parameters
conds_list
simList = conds_list[[1]]
conds = conds_to_load
#extract icc conditions
icc_conds = lapply(conds, function(x) parse_conds(x)[[1]])
true_params = list('sigma2' = 0.5, 'intercept' = 1, 'b_x' = 0.3,
'b_z' = 0.3, 'b_xz' = 0.3, 'int_var' = calc_var_comp_legacy(icc, .5)[1],
'x_var' = calc_var_comp_legacy(icc, .5)[1])
icc = 0.1
true_params = list('sigma2' = 0.5, 'intercept' = 1, 'b_x' = 0.3,
'b_z' = 0.3, 'b_xz' = 0.3, 'int_var' = calc_var_comp_legacy(icc, .5)[1],
'x_var' = calc_var_comp_legacy(icc, .5)[1])
true_params
x = simList
x = simList[[1]]
x$varComp[x$varComp$grp == 'Residual', 'vcov']
x
x$confInt['.sigma',]
x$varComp
x$fx_coefs
x$confInt
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['.sigma',])
ci_mat
x
x$varComp
x$fx_coefs
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$fx_coefs['x', 'Estimate'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['x',])
sims
ci_mat
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$fx_coefs['z', 'Estimate'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['z',])
sims
ci_mat
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$fx_coefs['xz', 'Estimate'])
x$fx_coefs
x$confInt
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$fx_coefs['x:z', 'Estimate'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['x:z',])
x$varComp
x$varComp$var1 == '(Intercept)'
x$varComp$var1 == '(Intercept)' & x$varComp$var2 == "<NA>"
x$varComp[1,3]
x$varComp[x$varComp$var1 == '(Intercept)' & is.na(x$varComp$var2), 'vcov']
x$varComp$var1 == '(Intercept)' & is.na(x$varComp$var2)
x$varComp[x$varComp$var1 == '(Intercept)' & is.na(x$varComp$var2), 'vcov']
x$varComp[x$varComp$var1 == '(Intercept)' & is.na(x$varComp$var2), 'vcov'][1]
x$varComp[x$varComp$var1 == 'x' & is.na(x$varComp$var2), 'vcov'][1]
x$confInt
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['.sigma',])
sims
ci_mat
View(ci_mat)
sims
x$varComp
x$confInt
sqrt(.68002747)
sqrt(.48756087)
.48 - .27
.48 - .21
.27 - .27
.21 - .27
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
#get the confidence interval values
ci_mat = sapply(simList, function(x) x$confInt['.sigma',]^2)
sims
ci_mat
View(ci_mat)
View(ci_mat)
x$confInt
x$varComp
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$var1 == 'x' & is.na(x$varComp$var2), 'vcov'][1])
#get the confidence interval values
#confidence intervals for variances are on SD scale, need to square to put back into variance
ci_mat = sapply(simList, function(x) x$confInt['.sig03',]^2)
sims
x$varComp
param_2_summarize = 'sigma2'
#extract the estimated values fit to the model from the simulated dataset
sims = sapply(simList, function(x) x$varComp[x$varComp$grp == 'Residual', 'vcov'])
#get the confidence interval values
#confidence intervals for variances are on SD scale, need to square to put back into variance
ci_mat = sapply(simList, function(x) x$confInt['.sigma',]^2)
#get the confidence interval values
#confidence intervals for variances are on SD scale, need to square to put back into variance
ci_mat = sapply(simList, function(x) x$confInt['.sigma',]^2)
bias = 100 * sims/true_params[[param_2_summarize]]
bias
bias = 100 * sims/true_params[[param_2_summarize]]; pct_bias = bias - 100 #compute percent bias
mean_pct_bias = mean(pct_bias) #get mean bias
mean_pct_bias
mean(bias)
mean(bias) + abs(mean_pct_bias)
hist = ggplot(as.data.frame(pct_bias), aes(x=pct_bias)) +
geom_histogram(aes(y=..density..), binwidth=1) + geom_vline(aes(xintercept=mean(pct_bias)), color = 'black', linetype="dashed", size = 1.25) +
geom_density(alpha = 0.2, fill = "green") #create histogram
hist
#get coverage values
coverage = sapply(1:1000, function(x) dplyr::between(true_params[[param_2_summarize]], ci_mat[1,x], ci_mat[2,x]))
#get coverage values
coverage = sapply(1:1000, function(x) dplyr::between(true_params[[param_2_summarize]], ci_mat[1,x], ci_mat[2,x]))
#calculate percent non-coverage
pct_noncoverage = 1 - mean(coverage, na.rm = T) #calculate percent noncoverage
coverage
pct_bias
pct_noncoverage
x
x$w
sapply(simList, function(x) x$w)
sum(sapply(simList, function(x) x$w))
